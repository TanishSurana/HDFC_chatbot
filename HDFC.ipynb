{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrDugHNfIIHV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Write a simple chatbot to query a large PDF document using LLMs  and return responses. Preferred language: Python.**\n"
      ],
      "metadata": {
        "id": "Q4BBPQVoIKoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ways to do it:\n",
        "\n",
        "1. using exsisting trained LLM: Bert or ChatGPT-3.5\n",
        "2. Build my own model? (Needs lots of data and time tho)"
      ],
      "metadata": {
        "id": "wxgxI6bBIXJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assumptions:\n",
        "Write a simple chatbot to query a large PDF document using LLMs  and return responses. Preferred language: Python.\n",
        "\n",
        "1. there is only one/single PDF document. Note we can make it work for multiple pdf documents.\n",
        "2. The PDF document is **not** a question answer reference, it is paragraph type document. From which my task is to make a bot that can answer questions about the documents such as information etc\n",
        "\n",
        "\n",
        "## model choices: LLM\n",
        "* GPT 3 to 4, are best for generating new text from the document (still with info). But requires api access == $$$\n",
        "\n",
        "* Bert: best for detail specific answering, can be finetuned on our document, only disadvatage: may struggle to create new text, as it can only refer a text.\n",
        "\n",
        "* T5: is a text to text LLM, so it is better for chatbots as it can answer in an human-like way. But only disadvantage is accuracy.\n",
        "\n",
        "> Bert also has variants: Lightweight versions, don't remember their names, same for t5 has better new models like flan-t5\n",
        "\n",
        "* XLNet: best for complex queries, but more expensive to train and use"
      ],
      "metadata": {
        "id": "6hkJrQzF0Gw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rough planning:\n",
        "\n",
        "* will use a sample pdf, and\n",
        "\n",
        "## problems with finetuning:\n",
        "* LLM if finetuned on a single PDF, for example a case study, they won't be able to learn the details out of it, its too sparse for it to learn, Will work if you have a large dataset.\n",
        "* **For small dataset, creating embeddings for the knowledge base is better**\n",
        "\n",
        "\n",
        "## Preprocessing the document: things to consider:\n",
        "* if the document is structured, i.e. is just text, like a wikipedia article\n",
        "* if the document has unstructured data, like tables or graphs, images\n",
        "* for tables, this step can be done in the preprocessing part, but for graphs and images more complex models will be required.\n",
        "\n",
        "\n",
        "\n",
        "## Metrics after deployment:\n",
        "* UER: User engagement rate, this is for the session's performance, how likely the users are using the bot\n",
        "* **fallback and containment rates**: % of queries solved by chatbot, can also include Handoff rates, % of the time the query was handed off to a human\n",
        "*  Others are there like: # of chats, # popular topics, # of avg questions, etc\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OMEE5Grg0pbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CODE:"
      ],
      "metadata": {
        "id": "ZfXynI8FYd7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HuggingFaceApi = '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' # should be in an .env file, but its a free account so doesn't matter for now"
      ],
      "metadata": {
        "id": "gpJN7noSYgVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting text from PDFs\n",
        "\n",
        "* first we will get text from our pdf file, then we will break the text into chunks (with overlap)\n",
        "\n",
        "* Then we will store it in a knowledge base, using langchain"
      ],
      "metadata": {
        "id": "I7Qst8-0gH7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjJPgKwrgqkN",
        "outputId": "ddb97174-558e-4bf8-c4a3-a2ca586afccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install InstructorEmbedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BasYy3s5z1kS",
        "outputId": "baf63434-6698-42e0-d1df-f9aa25e6cd58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: InstructorEmbedding in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6joufiH0le7",
        "outputId": "8a678a47-dc64-4d48-cff4-e5d6d111bf5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4OfBze9cejJ",
        "outputId": "1373703e-2e0f-46d5-9272-477e22fbb983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.340)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.66)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libs:\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS # makes embe\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings"
      ],
      "metadata": {
        "id": "QwAlJWABgeQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn7moSdKdVp7",
        "outputId": "cc1c7dae-c874-4446-e6b8-93159bd4f27f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get text from pdf\n",
        "def get_text(file_name):\n",
        "  text = ''\n",
        "  reader = PdfReader(file_name)\n",
        "  for page in reader.pages:\n",
        "    text += page.extract_text()\n",
        "\n",
        "  return text\n",
        "\n",
        "# get chunks (using chunk size of 512, common for most llm, also needs chunk)\n",
        "def get_chunks(text):\n",
        "  text_splitter = CharacterTextSplitter(\n",
        "      separator='\\n',\n",
        "      chunk_size = 512,\n",
        "      chunk_overlap = 200,\n",
        "      length_function = len,\n",
        "  )\n",
        "\n",
        "  chunks = text_splitter.split_text(text)\n",
        "  return chunks\n",
        "\n",
        "# get embeddings of the chunks adn return a knowledge base\n",
        "def get_embeddings(chunks):\n",
        "    emb = HuggingFaceInstructEmbeddings(model_name = \"hkunlp/instructor-xl\") # best hugginface model, but is slower, but okay for this one pdf.\n",
        "    know_base = FAISS.from_texts(texts=chunks, embedding = emb)\n",
        "    return know_base\n",
        "\n",
        "# testing and running part:\n",
        "#file_name = '/content/Sample.pdf'\n",
        "file_name = '/content/TanishDissertationFinal2.pdf'\n",
        "text = get_text(file_name) # this is raw text from the pdf, again mentioning the assumption the pdf is a simple text based otherwise for images, graphs more complex preprocessing is required\n",
        "chunks = get_chunks(text)\n",
        "kb = get_embeddings(chunks)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SG5M803SIWb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc637d0e-8e6a-49ec-fe82-e856dd81a8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import trange\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we have the instructor embeddings which are stored in kb\n",
        "print(kb)\n",
        "# we need to ask a query, then pass it through the same llm,\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuU3zmUDsjfl",
        "outputId": "0f5ec644-33c2-4290-df7b-1d35a2b9d743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<langchain.vectorstores.faiss.FAISS object at 0x7d8991cf68c0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_21g2F_c5Hkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling queries\n",
        "\n",
        "* so now we have a knowledge base, we can ask a query, then embedding the query and match it (cosine similarity) with our KB, this will give us the most important chunks."
      ],
      "metadata": {
        "id": "X4LYcXAVhiPp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4ASaUNlfyhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# to store the chat history and give context to the llm\n",
        "def chat_maker(kb):\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "    chats = ConversationalRetrievalChain.from_llm(\n",
        "        llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.2, \"max_length\":1000}, huggingfacehub_api_token='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'),\n",
        "        retriever=kb.as_retriever(),\n",
        "        memory=memory,\n",
        "    )\n",
        "    return chats\n",
        "\n"
      ],
      "metadata": {
        "id": "E1kj2FyXhcsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convo = chat_maker(kb) # kb is the knowledge base we created"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHc2V7uqhelI",
        "outputId": "28a8d957-c332-4b92-dd98-00838ebee452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('type \"exit\" to exit the chat')\n",
        "while True:\n",
        "  query = input(\"Ask a question about AI: \") # the document is about artificial intelligence\n",
        "  if query == 'exit':\n",
        "    break\n",
        "  print()\n",
        "  history = convo({'question': query})['chat_history'] # add question to conva and get the new chat history which has the new answer\n",
        "  print(history[-1].content)\n",
        "  print('\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSC_rYnotoq0",
        "outputId": "78116788-c472-4b05-c2e8-e11fcd53c330"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type \"exit\" to exit the chat\n",
            "Ask a question about AI: what is optical flow\n",
            "\n",
            "the apparent movement of objects, edges, and surfaces between consecutive video frames\n",
            "\n",
            "\n",
            "\n",
            "Ask a question about AI: what are the types?\n",
            "\n",
            "Sparse and Dense optical flow\n",
            "\n",
            "\n",
            "\n",
            "Ask a question about AI: What is sparse?\n",
            "\n",
            "Motion vectors are calculated for specific objects or features in the frame, i.e., not for each pixel in the image.\n",
            "\n",
            "\n",
            "\n",
            "Ask a question about AI: what is dense then?\n",
            "\n",
            "dense optical flow\n",
            "\n",
            "\n",
            "\n",
            "Ask a question about AI: What is the parallax?\n",
            "\n",
            "objects closer to a viewpoint have a faster perceived motion when compared to objects further away\n",
            "\n",
            "\n",
            "\n",
            "Ask a question about AI: what is raft?\n",
            "\n",
            "a machine learning model for estimating optical flow\n",
            "\n",
            "\n",
            "\n",
            "Ask a question about AI: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-aU262TgvxqV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}